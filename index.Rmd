---
title: "Tool for identifing replicate field study locations"
author: Mitchell Hitchcock
subtitle: "Demonstrated with Erie County brownfields"
output: html
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

Replicated experiments are well known to have the highest power for statistical inference. However, this has rarely been properly implemented in field studies and numerous techniques have been devised to alleviate this challenge while avoiding psuedoreplication (Eberhardt and Thomas 1991).   Due to the power (lower typeII error) of large replicate studies it would be desirable to develop a method for conducting such studies in field experiments so as to derive stronger inferences. Higher power is of equal interest to Bayesian analysis as it is to frequentest but discussion of either is outside the scope of this work save that it benefits all. The purpose of this project is to develop a tool for alleviating the first step which is identifying plausible replicate sites by using existing locations. It is possible to deal with uncontrolled variables through large numbers of replicates but the more uncontrolled and higher variation the variables are the more replicates are needed. This tool uses the large amounts of environmental data widely available to filter through locations and identify a desired number of replicates which are most similar given a set of variables. This tool is designed to be dynamic by allowing for multiple spatial data sets to be imported and desired variables to be filtered for. 



# Materials and methods

SHINY CODE GOES HERE

this will eventually be incorperated as a shiny app



# Results


```{r actual_run, echo=FALSE}

library(sf)
library(sp)
library(dplyr)
library(tidyverse)
library(doParallel)
library(nngeo)#st_nn
library(leaflet)


####user input data (demo below) would like to add in soils and climate data but for now KISS
##if zip unzip else
# temp <- tempfile()
# download.file("http://gis.ny.gov/gisdata/fileserver/?DSID=1300&file=Erie-Tax-Parcels-Centroid-Points-SHP.zip",temp)
# unzip(temp, exdir="data/Erie")
shapeobj1<-  st_read("data/Erie/Erie_2018_Tax_Parcel_Centroid_Points_SHP.shp")
# unlink(temp)


#download.file("https://data.ny.gov/api/views/c6ci-rzpg/rows.csv?accessType=DOWNLOAD&sorting=true","data/points.csv")
pointcoords<- read.csv("data/points.csv")


####convert data as necessary

  ############This is a workign first step
point_trn<-st_as_sf(pointcoords,coords=c("Longitude","Latitude")) %>% 
  st_set_crs(4326) %>%
  st_transform(st_crs(shapeobj1)) %>% 
  st_crop(st_bbox(shapeobj1))  






####join data to intersecting objects
  ###This would be followed by additional data sets in a for loop
      ####aggregate unions identical points collapsing certain factors into vectors of factor level

shapeobj2<-st_join(point_trn,shapeobj1,join=st_nn,k=1,maxdist=500) %>% 
  aggregate( by=list(.$Program.Facility.Name),FUN=unique,do_union=TRUE,join = st_intersects)
  



####User selects columns of interest and fixed value columns (done in shiny app)
  ##each column should be listed with a drop down menu for use, ignore, or fixed
    ##if fixed then set the fixed value from a list of possible values or factors
      ##this information is then used to filter (should probably have a submit button to avoid continuous run)
######DEMO all this here#####

shapeobj_trim<-select(shapeobj2,Program.Facility.Name,ACRES,Contaminants,Program.Type,DEC.Region,Control.Code,OU,Site.Class) %>% 
  filter(ACRES>0) %>% 
  mutate(Score=0)



rm(shapeobj1,shapeobj2,pointcoords,point_trn)

####user chooses number of sites desired "n"
number=5


####stepwise comparison of similarity (distance for values ) for all possible combinations of "n" sites
  ###this should be a simple /s algo generating a first list generating a score, generating a sequential list
    ###generating and comparing the two score, and keeping the better score and repeat with next sequential list

#####score difference function between two rows
  ###input rows need already to be filtered for desired test columns
    ####factors lost there levels somehow but the unique factor number remained in a vector see aggregate above
score.calc<-function(rowprime,rowtest){

  colscore<-vector(length=length(rowprime))
  for(i in 1:ncol(rowprime)){
    prime<-rowprime[i]
    if(is.list(prime)){
      prime<-unlist(prime)
    }
    test<-rowtest[i]
    if(is.list(test)){
      test<-unlist(test)
    }
    if(length(prime)>1||length(test)>1){
      colscore[i]<-as.double(length(c(
        setdiff(prime,test),
        setdiff(test,prime)))
      )

    }
    else if(is.double(prime)||is.numeric(prime)){
        colscore[i]<-as.double(abs((prime-test)/(prime+test)))

    }
      else if(is.factor(prime)||is.character(prime)){
        if(prime==test){
          colscore[i]<-0

        }
        else{
          colscore[i]<-1

        }
      }
      else 
        colscore[i]<-NA
        }

  return(sum(colscore))
}



##########run through each
registerDoParallel(cores = 6)

####Note that there should be no zero scores because every site has unique names this is ok because it is uniform
possible_n<- foreach(r=1:nrow(shapeobj_trim))%dopar%{
  x<-shapeobj_trim %>% 
    select(-geometry,-Score)
  x_ret<-shapeobj_trim
  for(i in 1:nrow(x)){
  
    x_ret[i,"Score"]<-score.calc(x[r,],x[i,])
    
  }
  
  x_ret<-head(x_ret[order(x_ret$Score),],n=number)
  x_ret
}



###establish values of smallest (best) value for comparisons in score across lists
###uses difference between of most different values to generate list. this could be improved
minimum<-foreach(i=1:length(possible_n),.combine=c)%dopar%{
  sum(possible_n[[i]]$Score)
  } %>% 
  min()


####generate a list of best combinations
best<-foreach(i=1:length(possible_n))%dopar%{
  if(sum(possible_n[[i]]$Score)==minimum){
    return(possible_n[[i]])
  }
} %>% 
  compact()


  ###the resulting combinations
####generated site list displayed in leaflet map and table 
  ### downloadable option could be nice too.

print("best possible combination of n sites")
best[[1]]
first<-st_transform(best[[1]],'+proj=longlat +datum=WGS84')
leaflet() %>% 
  addTiles() %>% 
  addCircleMarkers(data = st_geometry(first), popup=first$Program.Facility.Name)
####try data tables package




```


leaflet map and table will eventually go here



# Conclusions


# References

Eberhardt, L. L., and J. M. Thomas. 1991. Designing Environmental Field Studies. Ecological Monographs 61:53â€“73.
