---
title: "Tool for identifing replicate field study locations"
author: Mitchell Hitchcock
subtitle: "Demonstrated with Erie County brownfields"
output:
  html_document:
    code_folding: hide
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction


Replicated experiments are well known to have the highest power for statistical inference. However, this has rarely been properly implemented in field studies and numerous techniques have been devised to alleviate this challenge while avoiding psuedoreplication (Eberhardt and Thomas 1991). Due to the power (lower typeII error) of large replicate studies it would be desirable to develop a method for conducting such studies in field experiments so as to derive stronger inferences. Higher power is of equal interest to Bayesian analysis as it is to frequentest but discussion of either is outside the scope of this work save that it benefits all. The purpose of this project is to develop a tool for alleviating the first step which is identifying plausible replicate sites by using existing locations. It is possible to deal with uncontrolled variables through large numbers of replicates but the more uncontrolled and higher variation the variables are the more replicates are needed. This tool uses the large amounts of environmental data widely available to filter through locations and identify a desired number of replicates which are most similar given a set of variables. This tool is designed to be dynamic by allowing for multiple spatial data sets to be imported and desired variables to be filtered for. 



# Materials and methods


Datasets:

[Erie County tax parcels](http://gis.ny.gov/gisdata/fileserver/?DSID=1300&file=Erie-Tax-Parcels-Centroid-Points-SHP.zip)

[NYS Brownfields](https://data.ny.gov/api/views/c6ci-rzpg/rows.csv?accessType=DOWNLOAD&sorting=true","data/points.csv)

These datasets were chosen to deomnstrate this tool becuase they are readily available and are focus locally. The tax parcels data allows us to determine property ownsership, acreage, value, contact information, and other attributes. A better option that this point file is a shape file of the parcels which is available upon request from the county. A shape file would allow more accurate attribution of other variables such as soil and climate. The brownfields data is chosen for demonstrations of how this tool could be used to identify potential brownfields with similar enough variables to allow use as replicates in related studies. Additoinal data could come from USDA, USGS, NOAA, and NASA depending on variables of interest.


Shiny code (in progress version at https://mitchhitch.shinyapps.io/prjkt/)

```{r shiny_code, eval=FALSE}

##render is reactive



library(shiny)
library(leaflet)
library(dplyr)
library(tools)
library(sf)
library(sp)
library(tidyverse)
library(doParallel)
library(nngeo)#st_nn
library(stringr)



ui <- fluidPage(
    #Make into tabs
    navbarPage(
        "Best Rep",
        tabPanel("Step 1",
                 #Upload file1
                 h2('Upload your files'),
                 textOutput("at least one must be a .shp file (.csv with headers and .shp only"),
                 fileInput('file1', 'This must be a shp'),
                 #upload file2
                 fileInput('file2', 'Choose second file'),
                 #select number of reps desired
                 numericInput("n_reps","How many replicates would you like?",value=30,min=2,max=300)
                 ),
        
        tabPanel("Step 2",
                 
                 #Select which columns you care about backend in server
                 uiOutput("SelectCols1"),
                 uiOutput("SelectCols2")
                 
                 
                 ),
        tabPanel("Step 3",
                 #Fixed variables within the columns
                 
                 #Run Button
                 actionButton("RUN", "Click and walk away")
                 ),
        tabPanel("Results",
                 #leaflet
                 #Table
                 leafletOutput("bestmap")

                 
                 )
    )
    
    
    
    
)

        

server <- function(input, output,session) {
    options(shiny.maxRequestSize=30000*1024^2)
    
    #################funcz
    #####This should be run AFTER columns have been trimmed
    TransCoord_csv<-function(File1,FileX,FileX_Lon="Longitude",FileX_Lat="Latitude",FileX_crs=4326){
        
        FileX<-st_as_sf(FileX,coords=c(FileX_Lon,FileX_Lat)) %>% 
            st_set_crs(FileX_crs) %>% 
            st_transform(st_crs(File1)) %>% 
            st_crop(st_bbox(File1))
        FileX
        
    }
    
    
    ### Join all files into 1 sf ONLY run afer TransCoord_csv or st_transform for shape files.
    All_F<-function(File1,File2){
        # OtherF<-list(...)
        # countF<-length(OtherF)
        MainF<-File1
        # for(i in 1:countF){
        #     MainF=st_join(OtherF[[1]],MainF,join=st_nn,k=1,maxdist=500) %>% 
        #         aggregate( by=list(.[,1]),FUN=unique,do_union=TRUE,join = st_intersects)
        # }
        File2<-File2
        MainF=st_join(File2,MainF,join=st_nn,k=1,maxdist=500) %>% 
            aggregate( by=list(.$Program.Facility.Name),FUN=unique,do_union=TRUE,join = st_intersects)
        MainF
        
    }
    
    ###runs the itterative ranking
    Run.score<-function(shapeobj_trim,number){
        number=number
        foreach(r=1:nrow(shapeobj_trim))%dopar%{
            x<-shapeobj_trim %>% 
                select(-geometry)
            x_ret<-shapeobj_trim
            for(i in 1:nrow(x)){
                
                x_ret[i,"Score"]<-score.calc(x[r,],x[i,])
                
            }
            
            x_ret<-head(x_ret[order(x_ret$Score),],n=number)
            x_ret
        }
    }
    
    ###This ranks the similarit of rows
    score.calc<-function(rowprime,rowtest){
        
        colscore<-vector(length=length(rowprime))
        for(i in 1:ncol(rowprime)){
            prime<-rowprime[i]
            if(is.list(prime)){
                prime<-unlist(prime)
            }
            test<-rowtest[i]
            if(is.list(test)){
                test<-unlist(test)
            }
            if(length(prime)>1||length(test)>1){
                colscore[i]<-as.double(length(c(
                    setdiff(prime,test),
                    setdiff(test,prime)))
                )
                
            }
            else if(is.double(prime)||is.numeric(prime)){
                colscore[i]<-as.double(abs((prime-test)/(prime+test)))
                
            }
            else if(is.factor(prime)||is.character(prime)){
                if(prime==test){
                    colscore[i]<-0
                    
                }
                else{
                    colscore[i]<-1
                    
                }
            }
            else 
                colscore[i]<-NA
        }
        
        return(sum(colscore))
    }
    
    
    ###Select best sets of sites
    Out_best<-function(possible_n){
        #Determine min possible value for sets
        minimum<-foreach(i=1:length(possible_n),.combine=c)%dopar%{
            sum(possible_n[[i]]$Score)
        } %>% 
            min()
        #Select sets with min possible value
        best<-foreach(i=1:length(possible_n))%dopar%{
            if(sum(possible_n[[i]]$Score)==minimum){
                return(possible_n[[i]])
            }
        } %>% 
            compact()
        
        best
        
    }
    ##########################3
    ###File1
    data1<- reactive({
        withProgress(message = 'Loading Files', value = 0, {
            n<-4
            incProgress(1/n, detail = paste("Uploading", 1))
            filein <- input$file1
            if (is.null(filein)) { 
                return() 
            } 
            else if (file_ext(filein$datapath)=="csv"){
                incProgress(1/n, detail = paste("Reading csv", 2))
                data = read.csv(file=filein$datapath)
                incProgress(1/n, detail = paste("csv Read", 3))
                return(data)
            }
            else if (!is.null(filein)){
                incProgress(1/n, detail = paste("Unzipping", 2))    
                prevWD <- getwd()
                unlink("unzipped", recursive = TRUE)
                dir.create("unzipped")
                tempdir <- "unzipped"
                unzip(filein$datapath, exdir = tempdir)
                setwd(tempdir)
                if(length(list.dirs(recursive=FALSE))>0){
                    setwd(list.dirs(recursive = FALSE))
                }
                else{
                    print("in correct wd")
                }
                shpName <- list.files()[grep(list.files(),pattern="*.shp")[1]]
                incProgress(1/n, detail = paste("Reading shape file", 3))
                shpFile <- st_read(shpName)
                return(shpFile)
            } 
            else {
                return()
            }
            incProgress(1/n, detail = paste("Read", 4))
        })
        
    })
    
    
    ###File2
    data2 <- reactive({
        withProgress(message = 'Loading Files', value = 0, {
        n<-4
            incProgress(1/n, detail = paste("Uploading", 1))
        filein <- input$file2
        if (is.null(filein)) { 
            return() 
        } 
        else if (file_ext(filein$datapath)=="csv"){
            incProgress(1/n, detail = paste("Reading csv", 2))
            data = read.csv(file=filein$datapath)
            incProgress(1/n, detail = paste("csv Read", 3))
            return(data)
        }
        else if (!is.null(filein)){
            incProgress(1/n, detail = paste("Unzipping", 2))    
            prevWD <- getwd()
            unlink("unzipped", recursive = TRUE)
            dir.create("unzipped")
            tempdir <- "unzipped"
            unzip(filein$datapath, exdir = tempdir)
            setwd(tempdir)
            if(length(list.dirs(recursive=FALSE))>0){
                setwd(list.dirs(recursive = FALSE))
            }
            else{
                print("in correct wd")
            }
            shpName <- list.files()[grep(list.files(),pattern="*.shp")[1]]
            incProgress(1/n, detail = paste("Reading shape file", 3))
            shpFile <- st_read(shpName)
            return(shpFile)
        } 
        else {
            return()
            }
        incProgress(1/n, detail = paste("Read", 4))
        })

    })
 
    ##This is just a demo table for File1
    # output$table_display <- renderTable({
    #     f <- data1()
    #     f <- select(f, c(input$columns1)) #subsetting takes place here
    #     head(f)
    # })
    

    ###This allows for selecting desired columns
    ##File1
    output$SelectCols1 <- renderUI({
        selectInput("columns1", "Select Columns", choices= names(data1()),multiple = TRUE)
    })
    ##File2
    output$SelectCols2 <- renderUI({
        selectInput("columns2", "Select Columns", choices= names(data2()),multiple = TRUE)
    })
    
    
    
    RealRun<-eventReactive(input$RUN,{ 
        withProgress(message = 'Running Comparison', value = 0, {
        # Number of steps
        n <- 10
        
        req(input$columns1)
        req(input$columns2)
        number<-input$n_reps
        incProgress(1/n, detail = paste("Doing part", 1))
        registerDoParallel(cores = 6)
        incProgress(1/n, detail = paste("Doing part", 2))
        
        incProgress(1/n, detail = paste("Doing part", 3))
        
        data1_trim<-select(data1(), input$columns1)
        incProgress(1/n, detail = paste("Doing part", 4))
        rm(data1())
        data2_trim<-select(data2(), input$columns2)
        incProgress(1/n, detail = paste("Doing part", 5))
        rm(data2())
        data2_tran<-TransCoord_csv(data1_trim,data2_trim)
        incProgress(1/n, detail = paste("Doing part", 6))
        rm(data2_trim)
        data_ALL<-All_F(data1_trim,data2_tran)
        incProgress(1/n, detail = paste("Doing part", 7))
        rm(data1_trim,data2_tran)
        Ranked_list<-Run.score(data_ALL,number)
        incProgress(1/n, detail = paste("Doing part", 8))
        rm(data_ALL)
        best<-Out_best(Ranked_list)
        incProgress(1/n, detail = paste("Doing part", 9))
        rm(Ranked_list)
        first<-st_transform(best[[1]],'+proj=longlat +datum=WGS84')
        incProgress(1/n, detail = paste("Doing part", 10))
        
        first
        })
    })
    
    output$bestmap <- renderLeaflet({
        
        leaflet() %>% 
            addTiles() %>% 
            addCircleMarkers(data = st_geometry(RealRun()), popup=first$Program.Facility.Name)
        
    })
    
    
    #observe( on button press run the script from the values in place or set defaults.)


    
}


shinyApp(ui = ui, server = server)

```


Core Functions:

Transform crs and set bounding box to same area.
```{r transform, eval=FALSE}

TransCoord_csv<-function(File1,FileX,FileX_Lon="Longitude",FileX_Lat="Latitude",FileX_crs=4326){
  
  FileX<-st_as_sf(FileX,coords=c(FileX_Lon,FileX_Lat)) %>% 
    st_set_crs(FileX_crs) %>% 
    st_transform(st_crs(File1)) %>% 
    st_crop(st_bbox(File1))
  FileX
  
}

```

Join all uploaded files together.
```{r join, eval=FALSE}

All_F<-function(File1,...){
  OtherF<-list(...)
  countF<-length(OtherF)
  MainF<-File1
  for(i in 1:countF){
    MainF=st_join(OtherF[[1]],MainF,join=st_nn,k=1,maxdist=500) %>% 
      aggregate( by=list(.[,1]),FUN=unique,do_union=TRUE,join = st_intersects)
    
  }

}
```

Rank all the rows based on each field. This function compares each row with every other row and 
gives a score on how similar they are. Currently factors are weihted as 1 the same or 0 different.

```{r rank, eval=FALSE}

score.calc<-function(rowprime,rowtest){
  
  colscore<-vector(length=length(rowprime))
  for(i in 1:ncol(rowprime)){
    prime<-rowprime[i]
    if(is.list(prime)){
      prime<-unlist(prime)
    }
    test<-rowtest[i]
    if(is.list(test)){
      test<-unlist(test)
    }
    if(length(prime)>1||length(test)>1){
      colscore[i]<-as.double(length(c(
        setdiff(prime,test),
        setdiff(test,prime)))
      )
      
    }
    else if(is.double(prime)||is.numeric(prime)){
      colscore[i]<-as.double(abs((prime-test)/(prime+test)))
      
    }
    else if(is.factor(prime)||is.character(prime)){
      if(prime==test){
        colscore[i]<-0
        
      }
      else{
        colscore[i]<-1
        
      }
    }
    else 
      colscore[i]<-NA
  }
  
  return(sum(colscore))
}

```

Itterate over every possible combination and combine into a list of each sites best matches. This is defined
as the sites where their scores are the lowest when added together.
```{r itterate, eval=FALSE}

Run.score<-function(shapeobj_trim){
foreach(r=1:nrow(shapeobj_trim))%dopar%{
  x<-shapeobj_trim %>% 
    select(-geometry)
  x_ret<-shapeobj_trim
  for(i in 1:nrow(x)){
    
    x_ret[i,"Score"]<-score.calc(x[r,],x[i,])
    
  }
  
  x_ret<-head(x_ret[order(x_ret$Score),],n=number)
  x_ret
}
}
```

Select the best site combination (ties acceptable).
```{r best, eval=FALSE}

Out_best<-function(possible_n){
  #Determine min possible value for sets
minimum<-foreach(i=1:length(possible_n),.combine=c)%dopar%{
  sum(possible_n[[i]]$Score)
} %>% 
  min()
  #Select sets with min possible value
best<-foreach(i=1:length(possible_n))%dopar%{
  if(sum(possible_n[[i]]$Score)==minimum){
    return(possible_n[[i]])
  }
} %>% 
  compact()

best

}

```




# Results


```{r actual_run,  echo=FALSE, message=FALSE, warning=FALSE}

library(sf)
library(sp)
library(dplyr)
library(tidyverse)
library(doParallel)
library(nngeo)#st_nn
library(leaflet)
library(knitr)
library(DT)
library(xtable)
library(lwgeom)
library(FedData)
library(raster)
library(plyr)
library(raster)

####user input data (demo below) would like to add in soils and climate data but for now KISS
##if zip unzip else
# temp <- tempfile()
# download.file("http://gis.ny.gov/gisdata/fileserver/?DSID=1300&file=Erie-Tax-Parcels-Centroid-Points-SHP.zip",temp)
# unzip(temp, exdir="data/Erie")
#shapeobj1<-  st_read("data/Erie/Erie_2018_Tax_Parcel_Centroid_Points_SHP.shp")
# unlink(temp)
shapeobj1<- sf::st_read("data/eriecounty_parcels_2019_1119/eriecounty_parcels_1119.shp", quiet=TRUE)

#download.file("https://data.ny.gov/api/views/c6ci-rzpg/rows.csv?accessType=DOWNLOAD&sorting=true","data/points.csv")
pointcoords<- read.csv("data/points.csv")


shp<-st_make_valid(shapeobj1)

point_trn<-st_as_sf(pointcoords,coords=c("Longitude","Latitude")) %>% 
  st_set_crs(4326) %>%
  st_transform(st_crs(shp), quiet=TRUE) %>% 
  st_crop(st_bbox(shp)) 


gc(full=TRUE, verbose=FALSE)
####join data to intersecting objects must have shp first or geometry will be lost because
#multipolygon does not behave like polygon in st_contains where prepared=TRUE
  ###This would be followed by additional data sets in a for loop
      ####aggregate unions identical points collapsing certain factors into vectors of factor level

shapeobj2<-st_join(shp,point_trn,join=st_contains) %>%
  filter(!(is.na(.$Program.Facility.Name))) %>% 
  aggregate( by=list(.$Program.Facility.Name),FUN=unique,do_union=TRUE) %>% aggregate(by=list(.$GlobalID),FUN=unique,do_union=TRUE, join=st_instersects)

rm(shapeobj1, shp, point_trn, pointcoords)
gc(full=TRUE, verbose=FALSE)

#get percp data
GHCN.prcp <- get_ghcn_daily(template = shapeobj2,
                            label = "erie",
                            elements = c('TAVG','PRCP','TMAX','TMIN','TSUN'),
                            force.redo=F,years=1999:2019)

#average location percip data (could also group to monthly averages etc)
prcp<-GHCN.prcp$tabular %>%
    lapply("[[", "PRCP") %>%
    do.call("rbind",.) %>%
    gather(day, PRCP,-STATION, -YEAR, -MONTH) %>%
    filter(!is.na(PRCP)) %>%
    dplyr::select(-YEAR, -MONTH,-day) %>%
    aggregate(by=list(.$STATION),FUN=mean) %>%
    dplyr::select(-STATION)




#turns out most of this isnt availabel form erie county. 
# tavg<-GHCN.prcp$tabular %>%
#     lapply("[[", "TAVG") %>%
#     do.call("rbind",.) %>%
#     gather(day, TAVG,-STATION, -YEAR, -MONTH) %>%
#     filter(!is.na(TAVG)) %>%
#     dplyr::select(-YEAR, -MONTH,-day) %>%
#     aggregate(by=list(.$STATION),FUN=mean) %>%
#     dplyr::select(-STATION)





#reassign points to monthly averages
ghcn_pts<-st_as_sf(GHCN.prcp$spatial,
                   coords=c(GHCN.prcp$spatial@coords[,2],GHCN.prcp$spatial@coords[,1])
                  ) %>%
  st_transform(crs(shapeobj2)) %>%
  left_join(prcp,by=c("ID"="Group.1"),)

# write_sf(ghcn_pts,"ghcn_pts_file.shp")

ned_dat<-get_ned(template=shapeobj2, label="ned_elev", 
    res="1", force.redo = F)




##to avoid timely download
# ghcn_pts<-sf::st_read("ghcn_pts_file.shp", quiet=TRUE)
# 


####User selects columns of interest and fixed value columns (done in shiny app)
  ##each column should be listed with a drop down menu for use, ignore, or fixed
    ##if fixed then set the fixed value from a list of possible values or factors
      ##this information is then used to filter (should probably have a submit button to avoid continuous run)
######DEMO all this here#####
##also adds in precip data based on closest station ane average elevation


shapeobj_trim<-st_join(shapeobj2,ghcn_pts,join=st_nn,k=1, progress=FALSE) %>% 
  dplyr::select(Program.Facility.Name,PRCP,ASSESSACRE,Contaminants,Program.Type,DEC.Region,Control.Code,Site.Class) %>% 
  filter(ASSESSACRE>0) %>% 
  mutate(Score=0) %>% 
  mutate(elev=raster::extract(x=ned_dat,y=.,fun=mean))

  

rm(shapeobj2, prcp,ghcn_pts,GHCN.prcp, ned_dat)
gc(full=TRUE, verbose=FALSE)
####user chooses number of sites desired "n"
number=5


####stepwise comparison of similarity (distance for values ) for all possible combinations of "n" sites
  ###this should be a simple /s algo generating a first list generating a score, generating a sequential list
    ###generating and comparing the two score, and keeping the better score and repeat with next sequential list

#####score difference function between two rows
  ###input rows need already to be filtered for desired test columns
    ####factors lost there levels somehow but the unique factor number remained in a vector see aggregate above
score.calc<-function(rowprime,rowtest){

  colscore<-vector(length=length(rowprime))
  for(i in 1:ncol(rowprime)){
    prime<-rowprime[i]
    if(is.list(prime)){
      prime<-unlist(prime)
    }
    test<-rowtest[i]
    if(is.list(test)){
      test<-unlist(test)
    }
    if(length(prime)>1||length(test)>1){
      colscore[i]<-as.double(length(c(
        setdiff(prime,test),
        setdiff(test,prime)))
      )

    }
    else if(is.double(prime)||is.numeric(prime)){
        colscore[i]<-as.double(abs((prime-test)/(prime+test)))

    }
      else if(is.factor(prime)||is.character(prime)){
        if(prime==test){
          colscore[i]<-0

        }
        else{
          colscore[i]<-1

        }
      }
      else 
        colscore[i]<-NA
        }

  return(sum(colscore))
}



##########run through each
registerDoParallel(cores = 6)

####Note that there should be no zero scores because every site has unique names this is ok because it is uniform
possible_n<- foreach(r=1:nrow(shapeobj_trim))%dopar%{
  x<-shapeobj_trim %>% 
    dplyr::select(-geometry,-Score)
  x_ret<-shapeobj_trim
  for(i in 1:nrow(x)){
  
    x_ret[i,"Score"]<-score.calc(x[r,],x[i,])
    
  }
  
  x_ret<-head(x_ret[order(x_ret$Score),],n=number)
  gc(verbose=FALSE)
  return(x_ret)
}



###establish values of smallest (best) value for comparisons in score across lists
###uses difference between of most different values to generate list. this could be improved
minimum<-foreach(i=1:length(possible_n),.combine=c)%dopar%{
  sum(possible_n[[i]]$Score)
  } %>% 
  min()

gc()

####generate a list of best combinations
best<-foreach(i=1:length(possible_n))%dopar%{
  if(sum(possible_n[[i]]$Score)==minimum){
    return(possible_n[[i]])
  }
} %>% 
  compact()

gc(full=TRUE, verbose=FALSE)
  ###the resulting combinations
####generated site list displayed in leaflet map and table 
  ### downloadable option could be nice too.

first<-st_transform(best[[1]],'+proj=longlat +datum=WGS84')


leaflet() %>% 
  addTiles() %>%
  addPolygons(data = first, popup=first$Program.Facility.Name)
####try data tables package

datatable(first,
          filter = 'top', options = list(
            pageLength = number, autoWidth = TRUE,
            caption="This is a table of the best matched locations and thier corresponding variables"
          ))



```





# Conclusions

Although this tool is in very early stages it is promising. The flexibility of this tool will allow it to go beyond the example given here. Another possible use in consideration is identifying locations in a greenhouse with least amount of wind, light, and temperature variability. A major drawback is that this code as it stands is computationally intensive and prohibitive for use on personal computers. A solution to this would be to optimize the algorithms and/or host on a server.

# References

Eberhardt, L. L., and J. M. Thomas. 1991. Designing Environmental Field Studies. Ecological Monographs 61:53â€“73.

https://data.ny.gov/

http://gis.ny.gov/